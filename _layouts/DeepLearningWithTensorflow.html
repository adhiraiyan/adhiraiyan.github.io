<!DOCTYPE html>
<html>

{% include head.html %}

<body>
    <div id="wrapper">
        {% include header.html %}

        <!-------------------------------------- Page Content ------------------------------------>

        <div class="container">

            <h1 style="padding-bottom:40px;">Deep Learning with Tensorflow 2.0</h1>

            <img alt="Deep Learning with TF 2.0"
                src="/img/deeplearning/deeplearningwithtf/Deeplearningwithtensorflow2.png" align="left" width="200"
                style="padding-right: 20px;" />

            <p>
                A practical guide to Ian Goodfellow's <a href="https://www.deeplearningbook.org/">Deep Learning</a>
                book with Tensorflow 2.0 by Mukesh Mithrakumar. The content is available <a
                    href="https://github.com/adhiraiyan/DeepLearningWithTF2.0">on GitHub</a> and you can run it in <a
                    href="https://colab.research.google.com/github.com/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/00.00-Preface.ipynb">
                    Google Colaboratory</a> as well.
                The code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>.<br>
            </p>

            <p>
                If you find this content useful, consider buying me a cup of coffee ‚òïÔ∏èüòâ. <br>
            </p>
            <div style="padding-top:10px;">
                <span class="badge-buymeacoffee" style="padding-right:5px;"><a href="https://buymeacoffee.com/mmukesh"
                        title="Donate to this project using Buy Me A Coffee"><img
                            src="https://img.shields.io/badge/buy%20me%20a%20coffee-donate-blue.svg"
                            alt="Buy Me A Coffee donate button" /></a></span>
                <span class="badge-paypal" style="padding-right:5px;"><a href="https://paypal.me/mukeshmithrakumar"
                        title="Donate to this project using Paypal"><img
                            src="https://img.shields.io/badge/paypal-donate-green.svg"
                            alt="PayPal donate button" /></a></span>
                <span class="badge-patreon"><a href="https://patreon.com/mukeshmithrakumar"
                        title="Donate to this project using Patreon"><img
                            src="https://img.shields.io/badge/patreon-donate-orange.svg"
                            alt="Patreon donate button" /></a></span>
                <span class="liberpay badge" style="padding-right:5px;"><a
                        href="https://liberapay.com/mmukesh/donate"><img alt="Donate using Liberapay"
                            src="https://liberapay.com/assets/widgets/donate.svg"></a></span>
            </div>


            <h2 style="padding-top:150px;">Table of Contents</h2>

            <a href="00.00-Preface.ipynb">
                <h2>0. Preface</h2>
            </a>

            <a href="01.00-Introduction.ipynb">
                <h2>1. Introduction (05.05.2019)</h2>
            </a>
            <li>01.01 Who should read this book</li>
            <li>01.02 Historical Trends in Deep Learning</li>

            <a href="02.00-Linear-Algebra.ipynb">
                <h2>2. Applied Math and Machine Learning Basics (05.12.2019)</h2>
            </a>
            <li>02.01 Scalars, Vectors, Matrices and Tensors</li>
            <li>02.02 Multiplying Matrices and Vectors</li>
            <li>02.03 Identity and Inverse Matrices</li>
            <li>02.04 Linear Dependence and Span</li>
            <li>02.05 Norms</li>
            <li>02.06 Special Kinds of Matrices and Vectors</li>
            <li>02.07 Eigendecomposition</li>
            <li>02.08 Singular Value Decomposition</li>
            <li>02.09 The Moore-Penrose Pseudoinverse</li>
            <li>02.10 The Trace Operator</li>
            <li>02.11 The Determinant</li>
            <li>02.12 Example: Principal Components Analysis</li>


            <a href="03.00-Probability-and-Information-Theory.ipynb">
                <h2>3. Probability and Information Theory (05.19.2019)</h2>
            </a>
            <li>03.01 Why Probability?</li>
            <li>03.02 Random Variables</li>
            <li>03.03 Probability Distributions</li>
            <li>03.04 Marginal Probability</li>
            <li>03.05 Conditional Probability</li>
            <li>03.06 The Chain Rule of Conditional Probabilities</li>
            <li>03.07 Independence and Conditional Independence</li>
            <li>03.08 Expectation, Variance and Covariance</li>
            <li>03.09 Common Probability Distributions</li>
            <li>03.10 Useful Properties of Common Functions</li>
            <li>03.11 Bayes' Rule</li>
            <li>03.12 Technical Details of Continuous Variables</li>
            <li>03.13 Information Theory</li>
            <li>03.14 Structured Probabilistic Models</li>


            <a href="04.00-Numerical-Computation.ipynb">
                <h2>4. Numerical Computation (05.26.2019)</h2>
            </a>
            <li>04.01 Overflow and Underflow</li>
            <li>04.02 Poor Conditioning</li>
            <li>04.03 Gradient-Based Optimization</li>
            <li>04.04 Constrained Optimization</li>
            <li>04.05 Example: Linear Least Squares</li>


            <a href="05.00-Machine-Learning-Basics.ipynb">
                <h2>5. Machine Learning Basics (06.02.2019)</h2>
            </a>
            <li>05.01 Learning Algorithms</li>
            <li>05.02 Capacity, Overfitting and Underfitting</li>
            <li>05.03 Hyperparameters and Validation Sets</li>
            <li>05.04 Estimators, Bias and Variance</li>
            <li>05.05 Maximum Likelihood Estimation</li>
            <li>05.06 Bayesian Statistics</li>
            <li>05.07 Supervised Learning Algorithms</li>
            <li>05.08 Unsupervised Learning Algorithms</li>
            <li>05.09 Stochastic Gradient Descent</li>
            <li>05.10 Building a Machine Learning Algorithm</li>
            <li>05.11 Challenges Motivating Deep Learning</li>


            <a href="06.00-Deep-Feedforward-Networks.ipynb">
                <h2>6. Deep Feedforward Networks (06.09.2019)</h2>
            </a>
            <li>06.01 Example: Learning XOR</li>
            <li>06.02 Gradient-Based Learning</li>
            <li>06.03 Hidden Units</li>
            <li>06.04 Architecture Design</li>
            <li>06.05 Back-Propagation and Other Differentiation Algorithms</li>
            <li>06.06 Historical Notes</li>


            <a href="07.00-Regularization-for-Deep-Learning.ipynb">
                <h2>7. Regularization for Deep Learning (06.16.2019)</h2>
            </a>
            <li>07.01 Parameter Norm Penalties</li>
            <li>07.02 Norm Penalties as Constrained Optimization</li>
            <li>07.03 Regularization and Under-Constrained Problems</li>
            <li>07.04 Dataset Augmentation</li>
            <li>07.05 Noise Robustness</li>
            <li>07.06 Semi-Supervised Learning</li>
            <li>07.07 Multitask Learning</li>
            <li>07.08 Early Stopping</li>
            <li>07.09 Parameter Tying and Parameter Sharing</li>
            <li>07.10 Sparse Representations</li>
            <li>07.11 Bagging and Other Ensemble Methods</li>
            <li>07.12 Dropout</li>
            <li>07.13 Adversarial Training</li>
            <li>07.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier</li>


            <a href="08.00-Optimization-for-Training-Deep-Models.ipynb">
                <h2>8. Optimization for Training Deep Models (06.23.2019)</h2>
            </a>
            <li>08.01 How Learning Differs from Pure Optimization</li>
            <li>08.02 Challenges in Neural Network Optimization</li>
            <li>08.03 Basic Algorithms</li>
            <li>08.04 Parameter Initialization Strategies</li>
            <li>08.05 Algorithms with Adaptive Learning Rates</li>
            <li>08.06 Approximate Second-Order Methods</li>
            <li>08.07 Optimization Strategies and Meta-Algorithms</li>


            <a href="09.00-Convolutional-Networks.ipynb">
                <h2>9. Convolutional Networks (06.30.2019)</h2>
            </a>
            <li>09.01 The Convolution Operation</li>
            <li>09.02 Motivation</li>
            <li>09.03 Pooling</li>
            <li>09.04 Convolution and Pooling as an Infinitely Strong Prior</li>
            <li>09.05 Variants of the Basic Convolution Function</li>
            <li>09.06 Structured Outputs</li>
            <li>09.07 Data Types</li>
            <li>09.08 Efficient Convolution Algorithms</li>
            <li>09.09 Random or Unsupervised Features</li>
            <li>09.10 The Neuroscientific Basis for Convolutional Networks</li>
            <li>09.11 Convolutional Networks and the History of Deep Learning</li>


            <a href="10.00-Sequence-Modeling-Recurrent-and-Recursive-Nets.ipynb">
                <h2>10. Sequence Modeling: Recurrent and Recursive Nets (07.07.2019)</h2>
            </a>
            <li>10.01 Unfolding Computational Graphs</li>
            <li>10.02 Recurrent Neural Networks</li>
            <li>10.03 Bidirectional RNNs</li>
            <li>10.04 Encoder-Decoder Sequence-to-Sequence Architectures</li>
            <li>10.05 Deep Recurrent Networks</li>
            <li>10.06 Recursive Neural Networks</li>
            <li>10.07 The Challenge of Long-Term Dependencies</li>
            <li>10.08 Echo State Networks</li>
            <li>10.09 Leaky Units and Other Strategies for Multiple Time Scales</li>
            <li>10.10 The Long Short-Term Memory and Other Gated RNNs</li>
            <li>10.11 Optimization for Long-Term Dependencies</li>
            <li>10.12 Explicit Memory</li>


            <a href="11.00-Practical-Methodology.ipynb">
                <h2>11. Practical Methodology (07.14.2019)</h2>
            </a>
            <li>11.01 Performance Metrics</li>
            <li>11.02 Default Baseline Models</li>
            <li>11.03 Determining Whether to Gather More Data</li>
            <li>11.04 Selecting Hyperparameters</li>
            <li>11.05 Debugging Strategies</li>
            <li>11.06 Example: Multi-Digit Number Recognition</li>


            <a href="12.00-Applications.ipynb">
                <h2>12. Applications (07.21.2019)</h2>
            </a>
            <li>12.01 Large-Scale Deep Learning</li>
            <li>12.02 Computer Vision</li>
            <li>12.03 Speech Recognition</li>
            <li>12.04 Natural Language Processing</li>
            <li>12.05 Other Applications</li>


            <a href="13.00-Linear-Factor-Models.ipynb">
                <h2>13. Linear Factor Models (07.28.2019)</h2>
            </a>
            <li>13.01 Probabilistic PCA and Factor Analysis</li>
            <li>13.02 Independent Component Analysis</li>
            <li>13.03 Slow Feature Analysis</li>
            <li>13.04 Sparse Coding</li>
            <li>13.05 Manifold Interpretation of PCA</li>


            <a href="14.00-Autoencoders.ipynb">
                <h2>14. Autoencoders (08.04.2019)</h2>
            </a>
            <li>14.01 Undercomplete Autoencoders</li>
            <li>14.02 Regularized Autoencoders</li>
            <li>14.03 Representational Power, Layer Size and Depth</li>
            <li>14.04 Stochastic Encoders and Decoders</li>
            <li>14.05 Denoising Autoencoders</li>
            <li>14.06 Learning Manifolds with Autoencoders</li>
            <li>14.07 Contractive Autoencoders</li>
            <li>14.08 Predictive Sparse Decomposition</li>
            <li>14.09 Applications of Autoencoders</li>


            <a href="15.00-Representation-Learning.ipynb">
                <h2>15. Representation Learning (08.11.2019)</h2>
            </a>
            <li>15.01 Greedy Layer-Wise Unsupervised Pretraining</li>
            <li>15.02 Transfer Learning and Domain Adaptation</li>
            <li>15.03 Semi-Supervised Disentangling of Causal Factors</li>
            <li>15.04 Distributed Representation</li>
            <li>15.05 Exponential Gains from Depth</li>
            <li>15.06 Providing Clues to Discover Underlying Causes</li>


            <a href="16.00-Structured-Probabilistic-Models-for-Deep-Learning.ipynb">
                <h2>16. Structured Probabilistic Models for Deep Learning (08.18.2019)</h2>
            </a>
            <li>16.01 The Challenge of Unstructured Modeling</li>
            <li>16.02 Using Graphs to Describe Model Structure</li>
            <li>16.03 Sampling from Graphical Models</li>
            <li>16.04 Advantages of Structured Modeling</li>
            <li>16.05 Learning about Dependencies</li>
            <li>16.06 Inference and Approximate Inference</li>
            <li>16.07 The Deep Learning Approach to Structured Probabilistic Models</li>


            <a href="17.00-Monte-Carlo-Methods.ipynb">
                <h2>17. Monte Carlo Methods (08.25.2019)</h2>
            </a>
            <li>17.01 Sampling and Monte Carlo Methods</li>
            <li>17.02 Importance Sampling</li>
            <li>17.03 Markov Chain Monte Carlo Methods</li>
            <li>17.04 Gibbs Sampling</li>
            <li>17.05 The Challenge of Mixing between Separated Modes</li>


            <a href="18.00-Confronting-the-Partition-Function.ipynb">
                <h2>18. Confronting the Partition Function (09.01.2019)</h2>
            </a>
            <li>18.01 The Log-Likelihood Gradient</li>
            <li>18.02 Stochastic Maximum Likelihood and Contrastive Divergence</li>
            <li>18.03 Pseudolikelihood</li>
            <li>18.04 Score Matching and Ratio Matching</li>
            <li>18.05 Denoising Score Matching</li>
            <li>18.06 Noise-Contrastive Estimation</li>
            <li>18.07 Estimating the Partition Function</li>


            <a href="19.00-Approximate-Inference.ipynb">
                <h2>19. Approximate Inference (09.08.2019)</h2>
            </a>
            <li>19.01 Inference as Optimization</li>
            <li>19.02 Expectation Maximization</li>
            <li>19.03 MAP Inference and Sparse Coding</li>
            <li>19.04 Variational Inference and Learning</li>
            <li>19.05 Learned Approximate Inference</li>


            <a href="20.00-Deep-Generative-Models.ipynb">
                <h2>20. Deep Generative Models (09.15.2019)</h2>
            </a>
            <li>20.01 Boltzmann Machines</li>
            <li>20.02 Restricted Boltzmann Machines</li>
            <li>20.03 Deep Belief Networks</li>
            <li>20.04 Deep Boltzmann Machines</li>
            <li>20.05 Boltzmann Machines for Real-Valued Data</li>
            <li>20.06 Convolutional Boltzmann Machines</li>
            <li>20.07 Boltzmann Machines for Structured or Sequential Outputs</li>
            <li>20.08 Other Boltzmann Machines</li>
            <li>20.09 Back-Propagation through Random Operations</li>
            <li>20.10 Directed Generative Nets</li>
            <li>20.11 Drawing Samples from Autoencoders</li>
            <li>20.12 Generative Stochastic Networks</li>
            <li>20.13 Other Generation Schemes</li>
            <li>20.14 Evaluating Generative Models</li>
            <li>20.15 Conclusion</li>


            <hr>
            {% include footer.html %}
        </div>
</body>

</html>
