<!DOCTYPE html>
<html>

{% include head.html %}

<body>
    <div id="wrapper">
        {% include header.html %}

        <!-------------------------------------- Page Content ------------------------------------>

        <div class="container">

            <h1 style="padding-bottom:40px;">Deep Learning with Tensorflow 2.0</h1>

            <img alt="Deep Learning with TF 2.0"
                src="/img/deeplearning/deeplearningwithtf/DLBookCover1.png" align="left" width="220"
                style="padding-right: 20px;" />
            <p>
                This Book is a practical guide to <a
                    href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/Index.ipynb">
                    Deep Learning with Tensorflow 2.0</a>.
                The code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a> and is
                available for FREE <a href="https://github.com/adhiraiyan/DeepLearningWithTF2.0">on GitHub</a> and you
                can run the notebooks via
                <a
                    href="https://colab.research.google.com/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/Index.ipynb">
                    Google Colaboratory</a> as well.<br>
            </p>

            <p>
                If you find this content useful, consider buying me a cup of coffee ‚òïÔ∏èüòâ. <br>
            </p>
            <div style="padding-top:10px;">
                <style>
                    .bmc-button img {
                        width: 27px !important;
                        margin-bottom: 1px !important;
                        box-shadow: none !important;
                        border: none !important;
                        vertical-align: middle !important;
                    }

                    .bmc-button {
                        line-height: 36px !important;
                        height: 37px !important;
                        text-decoration: none !important;
                        display: inline-flex !important;
                        color: #ffffff !important;
                        background-color: #FF813F !important;
                        border-radius: 3px !important;
                        border: 1px solid transparent !important;
                        padding: 1px 9px !important;
                        font-size: 22px !important;
                        letter-spacing: 0.6px !important;
                        box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;
                        -webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;
                        margin: 0 auto !important;
                        font-family: 'Cookie', cursive !important;
                        -webkit-box-sizing: border-box !important;
                        box-sizing: border-box !important;
                        -o-transition: 0.3s all linear !important;
                        -webkit-transition: 0.3s all linear !important;
                        -moz-transition: 0.3s all linear !important;
                        -ms-transition: 0.3s all linear !important;
                        transition: 0.3s all linear !important;
                    }

                    .bmc-button:hover,
                    .bmc-button:active,
                    .bmc-button:focus {
                        -webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;
                        text-decoration: none !important;
                        box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;
                        opacity: 0.85 !important;
                        color: #ffffff !important;
                    }
                </style>
                <link href="https://fonts.googleapis.com/css?family=Cookie" rel="stylesheet"><a class="bmc-button"
                    target="_blank" href="https://www.buymeacoffee.com/mmukesh"><img
                        src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me a coffee"><span
                        style="margin-left:5px;">Buy me a coffee</span></a>
                <span class="liberpay badge" style="padding-right:5px; margin-left:10px;"><a
                        href="https://liberapay.com/mmukesh/donate"><img alt="Donate using Liberapay"
                            src="https://liberapay.com/assets/widgets/donate.svg"></a></span>
                <span class="badge-paypal" style="padding-left:10px;"><a href="https://paypal.me/mukeshmithrakumar"
                        title="Donate to this project using Paypal"><img
                            src="https://img.shields.io/badge/paypal-donate-green.svg"
                            alt="PayPal donate button" /></a></span>
            </div>

            <style>
                .offer {
                    color: #FF813F;
                    font-size: 2.0em;
                }
            </style>

            <p style="padding-top:30px;" class="offer">
                Early Access Offer: </p>
            <p>
                Sign up for Mentor subscription and get FOUR 1 hour 1 on 1 mentor sessions monthly to go through
                whatever questions you may have on the materials:
            </p>

            <span class="badge-patreon"><a href="https://www.patreon.com/bePatron?u=19664301"
                    title="Donate to this project using Patreon"><img src="/img/patron button.png" width="175"
                        alt="Patreon donate button" /></a></span>


            <h2 style="padding-top:60px;">Table of Contents</h2>

            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/Index.ipynb">
                <h2>0. Index</h2>
            </a>

            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/01.00-Introduction.ipynb">
                <h2>1. Introduction (05.05.2019)</h2>
            </a>
            <li>01.00 Preface</li>
            <li>01.01 Introduction</li>
            <li>01.02 Who should read this book</li>
            <li>01.03 A Short History of Deep Learning</li>

            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/02.00-Linear-Algebra.ipynb">
                <h2>2. Applied Math and Machine Learning Basics (05.12.2019)</h2>
            </a>
            <li>02.01 Scalars, Vectors, Matrices and Tensors</li>
            <li>02.02 Multiplying Matrices and Vectors</li>
            <li>02.03 Identity and Inverse Matrices</li>
            <li>02.04 Linear Dependence and Span</li>
            <li>02.05 Norms</li>
            <li>02.06 Special Kinds of Matrices and Vectors</li>
            <li>02.07 Eigendecomposition</li>
            <li>02.08 Singular Value Decomposition</li>
            <li>02.09 The Moore-Penrose Pseudoinverse</li>
            <li>02.10 The Trace Operator</li>
            <li>02.11 The Determinant</li>
            <li>02.12 Example: Principal Components Analysis</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/03.00-Probability-and-Information-Theory.ipynb">
                <h2>3. Probability and Information Theory (05.19.2019)</h2>
            </a>
            <li>03.01 Why Probability?</li>
            <li>03.02 Random Variables</li>
            <li>03.03 Probability Distributions</li>
            <li>03.04 Marginal Probability</li>
            <li>03.05 Conditional Probability</li>
            <li>03.06 The Chain Rule of Conditional Probabilities</li>
            <li>03.07 Independence and Conditional Independence</li>
            <li>03.08 Expectation, Variance and Covariance</li>
            <li>03.09 Common Probability Distributions</li>
            <li>03.10 Useful Properties of Common Functions</li>
            <li>03.11 Bayes' Rule</li>
            <li>03.12 Technical Details of Continuous Variables</li>
            <li>03.13 Information Theory</li>
            <li>03.14 Structured Probabilistic Models</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/04.00-Numerical-Computation.ipynb">
                <h2>4. Numerical Computation (05.26.2019)</h2>
            </a>
            <li>04.01 Overflow and Underflow</li>
            <li>04.02 Poor Conditioning</li>
            <li>04.03 Gradient-Based Optimization</li>
            <li>04.04 Constrained Optimization</li>
            <li>04.05 Example: Linear Least Squares</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/05.00-Machine-Learning-Basics.ipynb">
                <h2>5. Machine Learning Basics (06.02.2019)</h2>
            </a>
            <li>05.01 Learning Algorithms</li>
            <li>05.02 Capacity, Overfitting and Underfitting</li>
            <li>05.03 Hyperparameters and Validation Sets</li>
            <li>05.04 Estimators, Bias and Variance</li>
            <li>05.05 Maximum Likelihood Estimation</li>
            <li>05.06 Bayesian Statistics</li>
            <li>05.07 Supervised Learning Algorithms</li>
            <li>05.08 Unsupervised Learning Algorithms</li>
            <li>05.09 Stochastic Gradient Descent</li>
            <li>05.10 Building a Machine Learning Algorithm</li>
            <li>05.11 Challenges Motivating Deep Learning</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/06.00-Deep-Feedforward-Networks.ipynb">
                <h2>6. Deep Feedforward Networks (06.09.2019)</h2>
            </a>
            <li>06.01 Example: Learning XOR</li>
            <li>06.02 Gradient-Based Learning</li>
            <li>06.03 Hidden Units</li>
            <li>06.04 Architecture Design</li>
            <li>06.05 Back-Propagation and Other Differentiation Algorithms</li>
            <li>06.06 Historical Notes</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/07.00-Regularization-for-Deep-Learning.ipynb">
                <h2>7. Regularization for Deep Learning (06.16.2019)</h2>
            </a>
            <li>07.01 Parameter Norm Penalties</li>
            <li>07.02 Norm Penalties as Constrained Optimization</li>
            <li>07.03 Regularization and Under-Constrained Problems</li>
            <li>07.04 Dataset Augmentation</li>
            <li>07.05 Noise Robustness</li>
            <li>07.06 Semi-Supervised Learning</li>
            <li>07.07 Multitask Learning</li>
            <li>07.08 Early Stopping</li>
            <li>07.09 Parameter Tying and Parameter Sharing</li>
            <li>07.10 Sparse Representations</li>
            <li>07.11 Bagging and Other Ensemble Methods</li>
            <li>07.12 Dropout</li>
            <li>07.13 Adversarial Training</li>
            <li>07.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/08.00-Optimization-for-Training-Deep-Models.ipynb">
                <h2>8. Optimization for Training Deep Models (06.23.2019)</h2>
            </a>
            <li>08.01 How Learning Differs from Pure Optimization</li>
            <li>08.02 Challenges in Neural Network Optimization</li>
            <li>08.03 Basic Algorithms</li>
            <li>08.04 Parameter Initialization Strategies</li>
            <li>08.05 Algorithms with Adaptive Learning Rates</li>
            <li>08.06 Approximate Second-Order Methods</li>
            <li>08.07 Optimization Strategies and Meta-Algorithms</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/09.00-Convolutional-Networks.ipynb">
                <h2>9. Convolutional Networks (06.30.2019)</h2>
            </a>
            <li>09.01 The Convolution Operation</li>
            <li>09.02 Motivation</li>
            <li>09.03 Pooling</li>
            <li>09.04 Convolution and Pooling as an Infinitely Strong Prior</li>
            <li>09.05 Variants of the Basic Convolution Function</li>
            <li>09.06 Structured Outputs</li>
            <li>09.07 Data Types</li>
            <li>09.08 Efficient Convolution Algorithms</li>
            <li>09.09 Random or Unsupervised Features</li>
            <li>09.10 The Neuroscientific Basis for Convolutional Networks</li>
            <li>09.11 Convolutional Networks and the History of Deep Learning</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/10.00-Sequence-Modeling-Recurrent-and-Recursive-Nets.ipynb">
                <h2>10. Sequence Modeling: Recurrent and Recursive Nets (07.07.2019)</h2>
            </a>
            <li>10.01 Unfolding Computational Graphs</li>
            <li>10.02 Recurrent Neural Networks</li>
            <li>10.03 Bidirectional RNNs</li>
            <li>10.04 Encoder-Decoder Sequence-to-Sequence Architectures</li>
            <li>10.05 Deep Recurrent Networks</li>
            <li>10.06 Recursive Neural Networks</li>
            <li>10.07 The Challenge of Long-Term Dependencies</li>
            <li>10.08 Echo State Networks</li>
            <li>10.09 Leaky Units and Other Strategies for Multiple Time Scales</li>
            <li>10.10 The Long Short-Term Memory and Other Gated RNNs</li>
            <li>10.11 Optimization for Long-Term Dependencies</li>
            <li>10.12 Explicit Memory</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/11.00-Practical-Methodology.ipynb">
                <h2>11. Practical Methodology (07.14.2019)</h2>
            </a>
            <li>11.01 Performance Metrics</li>
            <li>11.02 Default Baseline Models</li>
            <li>11.03 Determining Whether to Gather More Data</li>
            <li>11.04 Selecting Hyperparameters</li>
            <li>11.05 Debugging Strategies</li>
            <li>11.06 Example: Multi-Digit Number Recognition</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/12.00-Applications.ipynb">
                <h2>12. Applications (07.21.2019)</h2>
            </a>
            <li>12.01 Large-Scale Deep Learning</li>
            <li>12.02 Computer Vision</li>
            <li>12.03 Speech Recognition</li>
            <li>12.04 Natural Language Processing</li>
            <li>12.05 Other Applications</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/13.00-Linear-Factor-Models.ipynb">
                <h2>13. Linear Factor Models (07.28.2019)</h2>
            </a>
            <li>13.01 Probabilistic PCA and Factor Analysis</li>
            <li>13.02 Independent Component Analysis</li>
            <li>13.03 Slow Feature Analysis</li>
            <li>13.04 Sparse Coding</li>
            <li>13.05 Manifold Interpretation of PCA</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/14.00-Autoencoders.ipynb">
                <h2>14. Autoencoders (08.04.2019)</h2>
            </a>
            <li>14.01 Undercomplete Autoencoders</li>
            <li>14.02 Regularized Autoencoders</li>
            <li>14.03 Representational Power, Layer Size and Depth</li>
            <li>14.04 Stochastic Encoders and Decoders</li>
            <li>14.05 Denoising Autoencoders</li>
            <li>14.06 Learning Manifolds with Autoencoders</li>
            <li>14.07 Contractive Autoencoders</li>
            <li>14.08 Predictive Sparse Decomposition</li>
            <li>14.09 Applications of Autoencoders</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/15.00-Representation-Learning.ipynb">
                <h2>15. Representation Learning (08.11.2019)</h2>
            </a>
            <li>15.01 Greedy Layer-Wise Unsupervised Pretraining</li>
            <li>15.02 Transfer Learning and Domain Adaptation</li>
            <li>15.03 Semi-Supervised Disentangling of Causal Factors</li>
            <li>15.04 Distributed Representation</li>
            <li>15.05 Exponential Gains from Depth</li>
            <li>15.06 Providing Clues to Discover Underlying Causes</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/16.00-Structured-Probabilistic-Models-for-Deep-Learning.ipynb">
                <h2>16. Structured Probabilistic Models for Deep Learning (08.18.2019)</h2>
            </a>
            <li>16.01 The Challenge of Unstructured Modeling</li>
            <li>16.02 Using Graphs to Describe Model Structure</li>
            <li>16.03 Sampling from Graphical Models</li>
            <li>16.04 Advantages of Structured Modeling</li>
            <li>16.05 Learning about Dependencies</li>
            <li>16.06 Inference and Approximate Inference</li>
            <li>16.07 The Deep Learning Approach to Structured Probabilistic Models</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/17.00-Monte-Carlo-Methods.ipynb">
                <h2>17. Monte Carlo Methods (08.25.2019)</h2>
            </a>
            <li>17.01 Sampling and Monte Carlo Methods</li>
            <li>17.02 Importance Sampling</li>
            <li>17.03 Markov Chain Monte Carlo Methods</li>
            <li>17.04 Gibbs Sampling</li>
            <li>17.05 The Challenge of Mixing between Separated Modes</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/18.00-Confronting-the-Partition-Function.ipynb">
                <h2>18. Confronting the Partition Function (09.01.2019)</h2>
            </a>
            <li>18.01 The Log-Likelihood Gradient</li>
            <li>18.02 Stochastic Maximum Likelihood and Contrastive Divergence</li>
            <li>18.03 Pseudolikelihood</li>
            <li>18.04 Score Matching and Ratio Matching</li>
            <li>18.05 Denoising Score Matching</li>
            <li>18.06 Noise-Contrastive Estimation</li>
            <li>18.07 Estimating the Partition Function</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/19.00-Approximate-Inference.ipynb">
                <h2>19. Approximate Inference (09.08.2019)</h2>
            </a>
            <li>19.01 Inference as Optimization</li>
            <li>19.02 Expectation Maximization</li>
            <li>19.03 MAP Inference and Sparse Coding</li>
            <li>19.04 Variational Inference and Learning</li>
            <li>19.05 Learned Approximate Inference</li>


            <a
                href="https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/20.00-Deep-Generative-Models.ipynb">
                <h2>20. Deep Generative Models (09.15.2019)</h2>
            </a>
            <li>20.01 Boltzmann Machines</li>
            <li>20.02 Restricted Boltzmann Machines</li>
            <li>20.03 Deep Belief Networks</li>
            <li>20.04 Deep Boltzmann Machines</li>
            <li>20.05 Boltzmann Machines for Real-Valued Data</li>
            <li>20.06 Convolutional Boltzmann Machines</li>
            <li>20.07 Boltzmann Machines for Structured or Sequential Outputs</li>
            <li>20.08 Other Boltzmann Machines</li>
            <li>20.09 Back-Propagation through Random Operations</li>
            <li>20.10 Directed Generative Nets</li>
            <li>20.11 Drawing Samples from Autoencoders</li>
            <li>20.12 Generative Stochastic Networks</li>
            <li>20.13 Other Generation Schemes</li>
            <li>20.14 Evaluating Generative Models</li>
            <li>20.15 Conclusion</li>


            <hr>
            {% include footer.html %}
        </div>
</body>

</html>
